# 2021.07

## 7/11

### Apache Spark 

It helps to run multiple processes concurrently. It is a more flexible version of Apache Hadoop, which had a limited interface with MapReduce. 

### Databricks 

Making Spark easier to use. 

#### What does Databricks offer?

* Clusters 
  * All-purpose clusters : running commands in the notebook 
  * Job clusters : specifically for running jobs fast and robust. When the job is complete, you cannot restart the job cluster 
* Jobs
* Folders (file system)
* Notebooks
* Libraries (ex. pandas) : the clusters remember which libraries were installed 
* Security 

Spark core engine : RDD 

Resilient Distributed Datasets (RDD) is a data structure used in Spark, which is distributed across many different server machines. 

## 7/22

매일 뭘 배우고 있는데 기록하는 걸 까먹는다....ㅠㅠ 

* Infrastructure as Code (IaC) : 회사의 인프라를 코드로 다 관리해서 언제든 쉽게 reproduce할 수 있고 기록이 남는다. ex. Terraform 

* IaaS : OS도 안주고 그냥 거의 하드웨어만 주는 서비스

* PaaS: OS, memory 같은걸 관리해주지만 앱은 스스로 만들어야하는 서비스 

* SaaS : 앱까지 다 줘서 거의 떠먹여주는 서비스 ㅎㅎ 

## 7/23

Airflow 스케쥴링을 할 떄 헷갈리는 부분 정리

* 가장 중요한 것은 실제로 내가 짠 task들이 도는 시간은 scheduling interval의 끝자락이라는 것. 즉, 매 시간 정시에 돌리는걸로 scheduling interval을 짰다면 8시에 도는 task는 7시에 start 된 job이다. 7시를 start_date이라고 부르고 8시를 execution_date 라고 부른다. 

* Airflow 코드에 start_date 를 명시해야한다. 이 시간은 최초로 언제 도는지에 영향을 주는데 그 start date에 실제로 코드가 돌지는 않는다. start_date는 과거로 또는 미래로 설정할 수 있는데 과거가 되었든 미래가 되었든 어쨋든 실제 코드가 도는 시간은 이 start_date 이후에 첫 scheduling interval에 부합하는 시간이다. 예를 들면, scheduling interval이 매주 월요일이라고 치자. 그리고 오늘은 7월 23일 금요일이다. 최초로 job들을 돌리고 싶은 날짜가 7월 26일 월요일이라면 start_date는 7월 19일 월요일 이전으로 설정해야한다. 왜냐면 7월 19일 월요일에 start 된 job이 execution date가 7월 26일로 될 것이기 때문에 만약 start date를 오늘인 23일로 한다면 26일에 도는게 아니라 26+7인 8월 2일 월요일에 최초로 돌게 된다. 
* catchup이라는 파라미터가 있는데 이걸 True라고 하면 start_date로부터 dag를 ON한 시점까지 돌아야했는데 못돈 애들을 ON하자마자 다 스케쥴링해버린다. catchup = False라고 하면 이걸 방지할 수 있다. 
* 정확히 어떤 특정한 날짜/시간에 최초로 실행시키고 싶다면 그 시간에서 scheduling interval을 빼서 적당히 잘 설정하면 되는 것 같다. 

<img src="/Users/kylee/Downloads/Data Engineering-28.jpg" alt="Data Engineering-28" style="zoom:50%;" />



## 7/24 

Database 공부.. 학부 떄 왜 열심히 안들었니? 재미없었지.. 

* Boyce-Codd Normal form 은 3NF + all determinants are candidate key여야한다. 
* normalization을 잘 해줄수록 anomaly가 안 일어나는법. 근데 또 normalization을 고집할 필요는 없다. 내 상황에서는 denormalized form이 더 효율적일 수 있다. 
* Foreign key를 어떤 테이블에 적용하느냐는 여러가지 디자인 초이스가 있겠지만 (1:1, 1:N, N:M이냐에 따라도 다름) 주로 그 foreign key value가 optional 한 쪽에 포함시켜주는 것이 좋다. 

Terraform 공부

* provider라는건 테라폼으로 구축해갈 인프라를 칭한다. 여기 네이버 클라우드도 있네 ㅎ 
* provisioning이라는 단어가 뭔가 했는데 비상사태?를 방지하기위한 관리라고 생각하면 될 것 같다. 예를 들어 누가 aws에 만들어둔 모든 인프라를 삭제하고 퇴사했다면 provisioning을 통해 축척해둔 코드만 있다면 거의 100프로 복구할 수 있다. 그래서 infrastructure as code 라는 뜻. 
